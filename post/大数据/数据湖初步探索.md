---
title: 数据湖初步探索
toc: true
categories:
  - 大数据
abbrlink: 89c758e
date: 2022-10-24 00:00:00
---
# 数据湖的概念
数据湖是一个存储库，存储的数据可以是任意格式、结构化、非结构化、半结构化数据，允许对数据进行加工。
数据湖从本质上来讲，是一种企业数据架构方法，物理上实现则是一个数据存储平台，用来集中化存储企业内海量的、多来源的、多种类的数据，并支持对数据进行快速加工和分析。

# 有了数据仓库为什么还要数据湖呢？
数据仓库发展史：
1. 离线数仓架构：所有数据通过Hive表存储在HDFS中，使用Hive SQL进行分析。分析的结果写到MySQL中，对外进行查询。
2. Lambda架构：<br>
    场景一：主要场景是离线分析场景，存在部分实时分析场景，离线分析 + 实时链路。
    场景二：离线场景和实时场景都比较多，构建离线数仓 + 实时数仓。
   Lambda架构存在的问题：
   - 离线数据和实时数仓同一个指标出来的数值存在不一致
   - 业务逻辑需要开发两遍
   - 分析数据需要存储两份
   
![Lambda架构](https://codedm.oss-cn-hangzhou.aliyuncs.com/images/20221024/1f9cba4f10354d42a45dbac0d1c6df62.png?x-oss-process=style/codedm)

3. Kappa架构：离线和实时一套架构（不要离线的了）一般是公司在实时分析的场景使用比较多才会采用Kappa架构。
   Kappa架构架构存在问题：
   - 所有中间分层数据存储在kafka中，无法使用SQL对分层数据进行即席查询。
   - Kafka中的数据存储是存在周期，并不是永久存储。
   - Kafka中的数据无法进行修改
   - 无法复用目前已经非常成熟的基于离线数仓的数据血缘、数据质量管理体系。需要重新实现一套数据血缘、数据质量管理体系。
   
![Kappa架构](https://codedm.oss-cn-hangzhou.aliyuncs.com/images/20221024/74b83f458b9744499a3e1aa01bdc4fd4.png?x-oss-process=style/codedm)
为了解决Kappa架构的痛点问题，业界最主流是采用“批流一体”方式，这里批流一体可以理解为批和流使用同一套SQL处理，也可以理解为处理框架的统一，例如：Spark、Flink，但这里更重要指的是存储层上的统一，只要存储层面上做到“批流一体”就可以解决以上Kappa遇到的各种问题。
数据湖技术可以很好的实现存储层面上的“批流一体”，这就是为什么大数据中需要数据湖的原因。

# 数据湖与数据仓库的区别

- 存储数据类型方面
  数据仓库是存储数据，进行建模，存储的是结构化数据；数据湖以其本源格式保存大量原始数据，包括结构化的、半结构化的和非结构化的数据，主要是由原始的、混乱的、非结构化的数据组成。在需要数据之前，没有定义数据结构和需求。
- 数据处理模式
  在我们可以加载到数据仓库中的数据时，我们首先需要定义好它，这叫做写时模式（Schema-On-Write）。
  而对于数据湖，您只需加载原始数据，然后，当您准备使用数据时，就给它一个定义，这叫做读时模式（Schema-On-Read）。这是两种截然不同的数据处理方法。因为数据湖是在数据使用时再定义模型结构，因此提高了数据模型定义的灵活性，可满足更多不同上层业务的高效率分析诉求。
![schema on read](https://codedm.oss-cn-hangzhou.aliyuncs.com/images/20221027/7f58b8fc201c4916918f40c8c2774b44.png?x-oss-process=style/codedm)
![schema on write](https://codedm.oss-cn-hangzhou.aliyuncs.com/images/20221027/93181cb1dfa243709eb819dc56dd6ef8.png?x-oss-process=style/codedm)

# 什么是 Hudi

Hudi 是由Uber开发并开源的Data Lakes解决方案。
Hudi能够基于HDFS之上管理大型分析数据集，可以对数据进行插入、更新、增量消费等操作，主要目的是高效减少摄取过程中的数据延迟。
![Hudi官方架构图](https://codedm.oss-cn-hangzhou.aliyuncs.com/images/20221028/0274df7b49774176b81b418556924832.png?x-oss-process=style/codedm)

## Hudi 的特性

- 通过索引插件快速的删除和Upset。
- 来自 Spark、Presto、Trino、Hive 等的 SQL 读/写
- 事务、回滚、并发控制。
- 流式摄取、内置 CDC 源和工具。
- 自动调整文件大小、数据集群、压缩、清理。
- 用于可扩展存储访问的内置元数据跟踪。
- 向后兼容的模式演变和实施。

# 什么是 Iceberg
Apache Iceberg是一种用于大型数据分析场景的开放表格式（Table Format）。
Iceberg使用一种类似于SQL表的高性能表格式，Iceberg格式表单表可以存储数十PB数据，适配Spark、Trino、PrestoDB、Flink和Hive等计算引擎提供高性能的读写和元数据管理功能，Iceberg是一种数据湖解决方案。

## Iceberg特性
- Iceberg支持实时/批量数据写入和读取，支持Spark/Flink计算引擎。
- Iceberg支持事务ACID,支持添加、删除、更新数据。
- 不绑定任何底层存储，支持Parquet、ORC、Avro格式兼容行存储和列存储。
- Iceberg支持隐藏分区和分区变更，方便业务进行数据分区策略。
- Iceberg支持快照数据重复查询，具备版本回滚功能。
- Iceberg扫描计划很快，读取表或者查询文件可以不需要分布式SQL引擎。
- Iceberg通过表元数据来对查询进行高效过滤。
- 基于乐观锁的并发支持，提供多线程并发写入能力并保证数据线性一致。